dataset_config:
  news_clippings:
    use_images: true
    use_features: false
    return_features_info: false
    processors:
      text_processor:
        type: clip_tokenizer
        params:
          max_seq_length: 77
      image_processor:
        type: torchvision_transforms
        params:
          transforms:
            - type: Resize
              params:
                size: [256, 256]
                interpolation: 3
            - type: CenterCrop
              params:
                size: [224, 224]
            - ToTensor
            - GrayScaleTo3Channels

model_config:
  clip:
    # Define parameters for the MLP classifier the
    # CLIP embeddings are fed into.
    num_labels: 2
    mlp_hidden_dim: 512
    mlp_num_layers: 2
    mlp_dropout: 0
    mlp_act: relu
    # Apply the normal lr for the classifier and
    # lr * finetune_lr_multiplier for all other layers.
    finetune_lr_multiplier: 1e-2
    training_head_type: classification
    losses:
    - type: cross_entropy
    # Choose whether to freeze the lower or all layers
    # of CLIP but not the classifier.
    freeze_lower: False
    freeze_all: False
    # Choose from options: vitb32, rn50, rn101.
    clip_model_type: ${env:model}

checkpoint:
  # Adjust the number of .ckpt files to save during training.
  # max_to_keep: 1
  pretrained_state_mapping:
    model.visual: model.visual
    model.transformer: model.transformer
    model.token_embedding: model.token_embedding
    model.ln_final: model.ln_final
    model.classifier: model.classifier